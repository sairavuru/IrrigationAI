{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tifffile as tiff\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(threshold=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def get_rand_patch(img, mask, sz=160):\n",
    "    \"\"\"\n",
    "    :param img: ndarray with shape (x_sz, y_sz, num_channels)\n",
    "    :param mask: binary ndarray with shape (x_sz, y_sz, num_classes)\n",
    "    :param sz: size of random patch\n",
    "    :return: patch with shape (sz, sz, num_channels)\n",
    "    \"\"\"\n",
    "    #print(len(img.shape), img.shape[0], img.shape[1], img.shape[0:1], mask.shape[0:1])\n",
    "    assert len(img.shape) == 3 and img.shape[0] > sz and img.shape[1] > sz and img.shape[0:2] == mask.shape[0:2]\n",
    "    xc = random.randint(0, img.shape[0] - sz)\n",
    "    yc = random.randint(0, img.shape[1] - sz)\n",
    "    patch_img = img[xc:(xc + sz), yc:(yc + sz)]\n",
    "    patch_mask = mask[xc:(xc + sz), yc:(yc + sz)]\n",
    "\n",
    "    # Apply some random transformations\n",
    "    random_transformation = np.random.randint(1,8)\n",
    "    if random_transformation == 1:  # reverse first dimension\n",
    "        patch_img = patch_img[::-1,:,:]\n",
    "        patch_mask = patch_mask[::-1,:,:]\n",
    "    elif random_transformation == 2:    # reverse second dimension\n",
    "        patch_img = patch_img[:,::-1,:]\n",
    "        patch_mask = patch_mask[:,::-1,:]\n",
    "    elif random_transformation == 3:    # transpose(interchange) first and second dimensions\n",
    "        patch_img = patch_img.transpose([1,0,2])\n",
    "        patch_mask = patch_mask.transpose([1,0,2])\n",
    "    elif random_transformation == 4:\n",
    "        patch_img = np.rot90(patch_img, 1)\n",
    "        patch_mask = np.rot90(patch_mask, 1)\n",
    "    elif random_transformation == 5:\n",
    "        patch_img = np.rot90(patch_img, 2)\n",
    "        patch_mask = np.rot90(patch_mask, 2)\n",
    "    elif random_transformation == 6:\n",
    "        patch_img = np.rot90(patch_img, 3)\n",
    "        patch_mask = np.rot90(patch_mask, 3)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return patch_img, patch_mask\n",
    "\n",
    "\n",
    "def get_patches_val():\n",
    "    trainIds = [str(i).zfill(2) for i in range(11, 12)]  # all availiable ids: from \"01\" to \"24\"\n",
    "    x_dict = dict()\n",
    "    y_dict = dict()\n",
    "    sz = 320\n",
    "    n_patches = 100000000\n",
    "    #print('Reading images')\n",
    "    for img_id in trainIds:\n",
    "        #print(tiff.imread('./Data/{}.tif'.format(img_id)).shape)\n",
    "        print(img_id + ' read')\n",
    "        img_m = normalize(tiff.imread('./Data/{}.tif'.format(img_id)))\n",
    "        #print(img_m.shape)\n",
    "        mask_stg1 = tiff.imread('./Label/{}.tif'.format(img_id))[:,:,np.newaxis] # / 255\n",
    "        mask_stg2 = mask_stg1 >0.\n",
    "        mask = mask_stg2.astype(int)\n",
    "        #print(mask.shape)\n",
    "        train_xsz = int(3/4 * img_m.shape[0])  # use 75% of image as train and 25% for validation\n",
    "        x_dict[img_id] = img_m[:, :, :]\n",
    "        y_dict[img_id] = mask[:, :, :]\n",
    "        #print(img_id + ' read')\n",
    "    x = list()\n",
    "    y = list()\n",
    "    total_patches = 0\n",
    "    while total_patches < n_patches:\n",
    "        #print(\"debug\")\n",
    "        img_id = random.sample(x_dict.keys(), 1)[0]\n",
    "        img = x_dict[img_id]\n",
    "        mask = y_dict[img_id]\n",
    "        img_patch, mask_patch = get_rand_patch(img, mask, sz)\n",
    "        x.append(img_patch)\n",
    "        y.append(mask_patch)\n",
    "        total_patches += 1\n",
    "    #print('Generated {} patches'.format(total_patches))\n",
    "        yield np.array(img_patch), np.array(mask_patch)\n",
    "        #yield np.array(x), np.array(y)\n",
    "        \n",
    "def get_patches():\n",
    "    trainIds = [str(i).zfill(2) for i in range(1, 11)]  # all availiable ids: from \"01\" to \"24\"\n",
    "    x_dict = dict()\n",
    "    y_dict = dict()\n",
    "    sz = 320\n",
    "    n_patches = 1000000000\n",
    "    #print('Reading images')\n",
    "    for img_id in trainIds:\n",
    "        print(img_id + ' read')\n",
    "        #print(tiff.imread('./Data/{}.tif'.format(img_id)).shape)\n",
    "        img_m = normalize(tiff.imread('./Data/{}.tif'.format(img_id)))\n",
    "        #print(img_m.shape)\n",
    "        mask_stg1 = tiff.imread('./Label/{}.tif'.format(img_id))[:,:,np.newaxis] # / 255\n",
    "        mask_stg2 = mask_stg1 >0.\n",
    "        mask = mask_stg2.astype(int)\n",
    "        #print(mask.shape)\n",
    "        train_xsz = int(3/4 * img_m.shape[0])  # use 75% of image as train and 25% for validation\n",
    "        x_dict[img_id] = img_m[:, :, :]\n",
    "        y_dict[img_id] = mask[:, :, :]\n",
    "        #print(img_id + ' read')\n",
    "    x = list()\n",
    "    y = list()\n",
    "    total_patches = 0\n",
    "    while total_patches < n_patches:\n",
    "        #print(\"debug\")\n",
    "        img_id = random.sample(x_dict.keys(), 1)[0]\n",
    "        img = x_dict[img_id]\n",
    "        mask = y_dict[img_id]\n",
    "        img_patch, mask_patch = get_rand_patch(img, mask, sz)\n",
    "        x.append(img_patch)\n",
    "        y.append(mask_patch)\n",
    "        total_patches += 1\n",
    "    #print('Generated {} patches'.format(total_patches))\n",
    "        yield np.array(img_patch), np.array(mask_patch)\n",
    "        #yield np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u-net model with up-convolution or up-sampling and weighted binary-crossentropy as loss func\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def unet_model(n_classes=1, im_sz=160, n_channels=3, n_filters_start=32, growth_factor=2, upconv=True,\n",
    "               class_weights=[0.1,0.9]):\n",
    "    droprate=0.3\n",
    "    n_filters = n_filters_start\n",
    "    inputs = Input((im_sz, im_sz, n_channels))\n",
    "    #inputs = BatchNormalization()(inputs)\n",
    "    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #pool1 = Dropout(droprate)(pool1)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    pool1 = BatchNormalization()(pool1)\n",
    "    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(droprate)(pool2)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    pool2 = BatchNormalization()(pool2)\n",
    "    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(droprate)(pool3)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    pool3 = BatchNormalization()(pool3)\n",
    "    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_0)\n",
    "    pool4_1 = MaxPooling2D(pool_size=(2, 2))(conv4_0)\n",
    "    pool4_1 = Dropout(droprate)(pool4_1)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    pool4_1 = BatchNormalization()(pool4_1)\n",
    "    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_1)\n",
    "    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2, 2))(conv4_1)\n",
    "    pool4_2 = Dropout(droprate)(pool4_2)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_2)\n",
    "    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up6_1 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv5), conv4_1])\n",
    "    else:\n",
    "        up6_1 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4_1])\n",
    "    up6_1 = BatchNormalization()(up6_1)\n",
    "    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_1)\n",
    "    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_1)\n",
    "    conv6_1 = Dropout(droprate)(conv6_1)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up6_2 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_1), conv4_0])\n",
    "    else:\n",
    "        up6_2 = concatenate([UpSampling2D(size=(2, 2))(conv6_1), conv4_0])\n",
    "    up6_2 = BatchNormalization()(up6_2)\n",
    "    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_2)\n",
    "    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_2)\n",
    "    conv6_2 = Dropout(droprate)(conv6_2)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up7 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_2), conv3])\n",
    "    else:\n",
    "        up7 = concatenate([UpSampling2D(size=(2, 2))(conv6_2), conv3])\n",
    "    up7 = BatchNormalization()(up7)\n",
    "    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = Dropout(droprate)(conv7)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up8 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv7), conv2])\n",
    "    else:\n",
    "        up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2])\n",
    "    up8 = BatchNormalization()(up8)\n",
    "    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = Dropout(droprate)(conv8)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up9 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv8), conv1])\n",
    "    else:\n",
    "        up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1])\n",
    "    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(n_classes, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "        class_loglosses = K.mean(K.binary_crossentropy(y_true, y_pred), axis=[0, 1, 2])\n",
    "        return K.sum(class_loglosses * K.constant(class_weights))\n",
    "\n",
    "    #model.compile(optimizer=Adam(lr=0.001), loss=weighted_binary_crossentropy)\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "    #print(K.eval(model.optimizer.lr))\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize(img):\n",
    "    min = img.min()\n",
    "    max = img.max()\n",
    "    x = 2.0 * (img - min) / (max - min) - 1.0\n",
    "    return x\n",
    "\n",
    "N_BANDS = 4\n",
    "N_CLASSES = 1  # Irrigated/Non Irrigated\n",
    "CLASS_WEIGHTS = [0.1,0.9]\n",
    "N_EPOCHS = 100\n",
    "UPCONV = True\n",
    "PATCH_SZ = 320  # should divide by 16\n",
    "BATCH_SIZE = 150\n",
    "TRAIN_SZ = 10000  # train size\n",
    "VAL_SZ = 1000    # validation size\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    return unet_model(N_CLASSES, PATCH_SZ, n_channels=N_BANDS, upconv=UPCONV, class_weights=CLASS_WEIGHTS)\n",
    "\n",
    "\n",
    "weights_path = 'weights'\n",
    "if not os.path.exists(weights_path):\n",
    "    os.makedirs(weights_path)\n",
    "weights_path += '/unet_weights.hdf5'\n",
    "\n",
    "# trainIds = [str(i).zfill(2) for i in range(1, 4)]  # all availiable ids: from \"01\" to \"24\"\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     X_DICT_TRAIN = dict()\n",
    "#     Y_DICT_TRAIN = dict()\n",
    "#     X_DICT_VALIDATION = dict()\n",
    "#     Y_DICT_VALIDATION = dict()\n",
    "\n",
    "#     print('Reading images')\n",
    "#     for img_id in trainIds:\n",
    "#         print(tiff.imread('./Data/{}.tif'.format(img_id)).shape)\n",
    "#         img_m = normalize(tiff.imread('./Data/{}.tif'.format(img_id)))\n",
    "#         print(img_m.shape)\n",
    "#         mask_stg1 = tiff.imread('./Label/{}.tif'.format(img_id))[:,:,np.newaxis] # / 255\n",
    "#         mask_stg2 = mask_stg1 >0.\n",
    "#         mask = mask_stg2.astype(int)\n",
    "#         print(mask.shape)\n",
    "#         train_xsz = int(3/4 * img_m.shape[0])  # use 75% of image as train and 25% for validation\n",
    "#         X_DICT_TRAIN[img_id] = img_m[:, :, :]\n",
    "#         Y_DICT_TRAIN[img_id] = mask[:, :, :]\n",
    "#         X_DICT_VALIDATION[img_id] = img_m[:, :, :]\n",
    "#         Y_DICT_VALIDATION[img_id] = mask[:, :, :]\n",
    "#         #X_DICT_TRAIN[img_id] = img_m[:train_xsz, :, :]\n",
    "#         #Y_DICT_TRAIN[img_id] = mask[:train_xsz, :, :]\n",
    "#         #X_DICT_VALIDATION[img_id] = img_m[train_xsz:, :, :]\n",
    "#         #Y_DICT_VALIDATION[img_id] = mask[train_xsz:, :, :]\n",
    "#         print(img_id + ' read')\n",
    "#     print('Images were read')\n",
    "\n",
    "def train_net():\n",
    "    print(\"start train net\")\n",
    "    dataset = tf.data.Dataset.from_generator(get_patches, (tf.float32, tf.int16))\n",
    "    dataset_val = tf.data.Dataset.from_generator(get_patches_val, (tf.float32, tf.int16))\n",
    "    #iterator = dataset.make_one_shot_iterator()\n",
    "    #dataset = dataset.batch(10)\n",
    "    #x,y = iterator.get_next()\n",
    "    #print(tf.shape(x))\n",
    "    model = get_model()\n",
    "    model_checkpoint = ModelCheckpoint(weights_path, verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5, min_lr=0.0001)\n",
    "    #model_checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', save_best_only=True)\n",
    "    csv_logger = CSVLogger('log_unet.csv', append=True, separator=';')\n",
    "    tensorboard = TensorBoard(log_dir='./tensorboard_unet', write_graph=True, write_images=True)\n",
    "    results = model.fit_generator(dataset.batch(32),steps_per_epoch=10000,epochs=200,\n",
    "                verbose=2, shuffle=True,\n",
    "                callbacks=[model_checkpoint, csv_logger, tensorboard, \n",
    "                           #early_stopping,\n",
    "                           reduce_lr],\n",
    "                validation_data= dataset_val.batch(32), validation_steps = 100)    \n",
    "        \n",
    "    return \n",
    "\n",
    "train_net()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "\n",
    "def normalize(img):\n",
    "    min = img.min()\n",
    "    max = img.max()\n",
    "    x = 2.0 * (img - min) / (max - min) - 1.0\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "N_BANDS = 4\n",
    "N_CLASSES = 1  # Irrigated/Non Irrigated\n",
    "CLASS_WEIGHTS = [0.1,0.9]\n",
    "N_EPOCHS = 100\n",
    "UPCONV = True\n",
    "PATCH_SZ = 320  # should divide by 16\n",
    "\n",
    "\n",
    "\n",
    "def predict(x, model, patch_sz=160, n_classes=1):\n",
    "    img_height = x.shape[0]\n",
    "    img_width = x.shape[1]\n",
    "    n_channels = x.shape[2]\n",
    "    print(img_height, img_width)\n",
    "    # make extended img so that it contains integer number of patches\n",
    "    npatches_vertical = math.ceil(img_height / patch_sz)\n",
    "    npatches_horizontal = math.ceil(img_width / patch_sz)\n",
    "    #print(npatches_vertical,npatches_horizontal)\n",
    "    extended_height = patch_sz * npatches_vertical\n",
    "    extended_width = patch_sz * npatches_horizontal\n",
    "    ext_x = np.zeros(shape=(extended_height, extended_width, n_channels), dtype=np.float32)\n",
    "     # fill extended image with mirrors:\n",
    "    ext_x[:img_height, :img_width, :] = x\n",
    "    for i in range(img_height, extended_height):\n",
    "        ext_x[i, :, :] = ext_x[2 * img_height - i - 1, :, :]\n",
    "    for j in range(img_width, extended_width):\n",
    "        ext_x[:, j, :] = ext_x[:, 2 * img_width - j - 1, :]\n",
    "\n",
    "    # now we assemble all patches in one array\n",
    "    patches_list = []\n",
    "    for i in range(0, npatches_vertical):\n",
    "        for j in range(0, npatches_horizontal):\n",
    "            x0, x1 = i * patch_sz, (i + 1) * patch_sz\n",
    "            y0, y1 = j * patch_sz, (j + 1) * patch_sz\n",
    "            #print(x0,x1,y0,y1)\n",
    "            patches_list.append(ext_x[x0:x1, y0:y1, :])\n",
    "    # model.predict() needs numpy array rather than a list\n",
    "    patches_array = np.asarray(patches_list)\n",
    "    # predictions:\n",
    "    patches_predict = model.predict(patches_array, batch_size=4)\n",
    "    prediction = np.zeros(shape=(extended_height, extended_width, n_classes), dtype=np.float32)\n",
    "    #print(prediction.shape, patches_predict.shape)\n",
    "    for k in range(patches_predict.shape[0]):\n",
    "        i = k // npatches_horizontal\n",
    "        j = k % npatches_vertical\n",
    "        x0, x1 = i * patch_sz, (i + 1) * patch_sz\n",
    "        y0, y1 = j * patch_sz, (j + 1) * patch_sz\n",
    "        #print(x0,x1,y0,y1,i,j,k, npatches_horizontal, npatches_vertical)\n",
    "        prediction[x0:x1, y0:y1, :] = patches_predict[k, :, :, :]\n",
    "    return prediction[:img_height, :img_width, :]\n",
    "\n",
    "\n",
    "def picture_from_mask(mask, threshold=0):\n",
    "    colors = {\n",
    "        0: [150, 150, 150],  # Land\n",
    "        1: [223, 194, 125],\n",
    "        #,  # rainfed\n",
    "       2: [27, 120, 55]    # Irrigated\n",
    "    }\n",
    "    z_order = {\n",
    "        1: 0,\n",
    "        2: 1\n",
    "        #,\n",
    "        #3: 0\n",
    "    }\n",
    "    pict = 255*np.ones(shape=(3, mask.shape[1], mask.shape[2]), dtype=np.uint8)\n",
    "    for ch in range(3):\n",
    "            pict[ch,:,:][mask[0,:,:] > threshold] = colors[1][ch]\n",
    "    #pict(1,:,:)[mask[1,:,:]> threshold] =colors[0][0] \n",
    "    #for i in range(1, 3):\n",
    "    #    cl = z_order[i]\n",
    "    #    for ch in range(2):\n",
    "    #        pict[ch,:,:][mask[cl,:,:] > threshold] = colors[cl][ch]\n",
    "    return pict\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = get_model()\n",
    "    model.load_weights(weights_path)\n",
    "    test_id = '09'\n",
    "    img = normalize(tiff.imread('./Data/{}.tif'.format(test_id)))   # make channels last\n",
    "    \n",
    "    for i in range(0):\n",
    "        if i == 0:  # reverse first dimension\n",
    "            mymat = predict(img[::-1,:,:], model, patch_sz=PATCH_SZ, n_classes=N_CLASSES).transpose([2,0,1])\n",
    "            #print(mymat[0][0][0], mymat[3][12][13])\n",
    "            #print(\"Case 1\",img.shape, mymat.shape)\n",
    "        elif i == 1:    # reverse second dimension\n",
    "            temp = predict(img[:,::-1,:], model, patch_sz=PATCH_SZ, n_classes=N_CLASSES).transpose([2,0,1])\n",
    "            #print(temp[0][0][0], temp[3][12][13])\n",
    "            #print(\"Case 2\", temp.shape, mymat.shape)\n",
    "            mymat = np.mean( np.array([ temp[:,::-1,:], mymat ]), axis=0 )\n",
    "        elif i == 2:    # transpose(interchange) first and second dimensions\n",
    "            temp = predict(img.transpose([1,0,2]), model, patch_sz=PATCH_SZ, n_classes=N_CLASSES).transpose([2,0,1])\n",
    "            #print(temp[0][0][0], temp[3][12][13])\n",
    "            #print(\"Case 3\", temp.shape, mymat.shape)\n",
    "            mymat = np.mean( np.array([ temp.transpose(0,2,1), mymat ]), axis=0 )\n",
    "        elif i == 3:\n",
    "            temp = predict(np.rot90(img, 1), model, patch_sz=PATCH_SZ, n_classes=N_CLASSES)\n",
    "            #print(temp.transpose([2,0,1])[0][0][0], temp.transpose([2,0,1])[3][12][13])\n",
    "            #print(\"Case 4\", temp.shape, mymat.shape)\n",
    "            mymat = np.mean( np.array([ np.rot90(temp, -1).transpose([2,0,1]), mymat ]), axis=0 )\n",
    "        elif i == 4:\n",
    "            temp = predict(np.rot90(img,2), model, patch_sz=PATCH_SZ, n_classes=N_CLASSES)\n",
    "            #print(temp.transpose([2,0,1])[0][0][0], temp.transpose([2,0,1])[3][12][13])\n",
    "            #print(\"Case 5\", temp.shape, mymat.shape)\n",
    "            mymat = np.mean( np.array([ np.rot90(temp,-2).transpose([2,0,1]), mymat ]), axis=0 )\n",
    "        elif i == 5:\n",
    "            temp = predict(np.rot90(img,3), model, patch_sz=PATCH_SZ, n_classes=N_CLASSES)\n",
    "            #print(temp.transpose([2,0,1])[0][0][0], temp.transpose([2,0,1])[3][12][13])\n",
    "            #print(\"Case 6\", temp.shape, mymat.shape)\n",
    "            mymat = np.mean( np.array([ np.rot90(temp, -3).transpose(2,0,1), mymat ]), axis=0 )\n",
    "        else:\n",
    "            temp = predict(img, model, patch_sz=PATCH_SZ, n_classes=N_CLASSES).transpose([2,0,1])\n",
    "            #print(temp[0][0][0], temp[3][12][13])\n",
    "            #print(\"Case 7\", temp.shape, mymat.shape)\n",
    "            mymat = np.mean( np.array([ temp, mymat ]), axis=0 )\n",
    "    \n",
    "    mymat = predict(img, model, patch_sz=PATCH_SZ, n_classes=N_CLASSES).transpose([2,0,1])\n",
    "    map = picture_from_mask(mymat, 0.6)\n",
    "    #mask = predict(img, model, patch_sz=PATCH_SZ, n_classes=N_CLASSES).transpose([2,0,1])  # make channels first\n",
    "    #map = picture_from_mask(mask, 0.5)\n",
    "    #tiff.imsave('result.tif', (255*mask).astype('uint8'))\n",
    "    tiff.imsave('result.tif', mymat)\n",
    "    tiff.imsave('map.tif', map)\n",
    "    print(\"Prediction Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
